Thinking1：什么是反向传播中的链式法则
答：神经网络中，每一层元素通过连接关系及连接权重向下一层传递，这是前向计算过程。当得到输出结果时，结果与真实值之间必然存在一定的误差，而误差沿着各层元素之间的连接和连接的导数，
   从后往前逐层传递，就是反向传播。“链式”关系则体现在反向传播的过程是层层相乘，不能跳过中间某一层进行。

Thinking2：请列举几种常见的激活函数，激活函数有什么作用
答：激活函数模拟的是神经元内部对输入信息的反应处理，激活函数本身也有兴奋区域和迟钝区域，也是模拟神经元对外界刺激不同所作出的反应不同。
    神经网络模型中，直观看来激活函数是对线性输入作出非线性反应。
    常见的激活函数有sigmoid，tanh，relu等，其中relu目前更为常用，因为其对于大于零的输入有恒定的输出，因此梯度恒为定值，由此可以避免梯度爆炸或梯度消失。

Thinking3：利用梯度下降法训练神经网络，发现模型loss不变，可能有哪些问题？怎么解决？
答：loss不变有两种可能，一种是代码错误，没有能够对学习的参数进行更新，此时需要修改代码错误即可；另一种情况是可能发生了梯度消失，即梯度变化太小，以至于参数没有变化，而使得loss不变
    这种情况可以调整神经网络的结构，如减少层数，还可以改变激活函数，如relu。

